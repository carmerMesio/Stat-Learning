---
title: "Task 3"
author: "David Cardoner & Arnau Mercader"
date: ''
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE,fig.height = 4.5, fig.width = 6,comment='',warning=FALSE,fig.align="center",
                      message=FALSE)
```

### Ap 1.  

For the Boston House-price corrected dataset use Lasso estimation (in glmnet) to fit the regression model where the response is CMEDV (the corrected version of MEDV) and the explanatory variables are the remaining 13 variables in the previous list. Try to provide an interpretation to the estimated model. Note: We have some extra data related with coordinates (boston.utm dataset), we merge it with Boston data for gain two extra features. Look first rows of data:

```{r,echo=FALSE}
library(MASS)
library(glmnet)
library(ggplot2)
library(dplyr)
library(tidyr)
library(caret)
library(knitr)
library(caTools)

# change directory of your .Rdata
load("D:/Users/str_aml/Desktop/task3 - Statistical Learning/boston.Rdata")

## features and response: CMEDV
matches <- c("CRIM","ZN","INDUS","CHAS","NOX","RM","AGE","DIS","RAD","TAX","PTRATIO",
             "B","LSTAT","CMEDV")

Boston <- boston.c[,matches]

# merge with coordinates features

Boston$coordx <- boston.utm[,1]

Boston$coordy <- boston.utm[,2]


head(Boston)



```


First we are going to explore the response **CMDEV** (corrected variable) and apply some transformation. We are not going to scale the response because we will use intercept in Lasso regression model __(glmnet)__ (the best normalization is using log transformation). We consider hot encoding in **RAD** feature and to illustrate we compare results with __caret__, another great package for machine learning problems with R.


```{r}

Boston %>% mutate(log_CMDEV=log(CMEDV),
scale_CMEDV = CMEDV-mean(CMEDV)) %>%
gather(respuesta,valor,c(log_CMDEV,CMEDV,scale_CMEDV)) %>%
ggplot(.,aes(valor,fill=as.factor(respuesta))) +
geom_histogram(bins=30,colour='black') + guides(fill=FALSE) +
ggtitle("CMEDV, log(CMEDV), scale(CMEDV)") + facet_wrap(~respuesta,scales = "free")

```


```{r}
# Define of caret control
set.seed(123)
CARET.TRAIN.CTRL <- trainControl(method="cv",
                                 number = 10,
                                 verboseIter=FALSE)
```


We take a look at the data variables. Find NA's and categorical encoding for variable rad.

```{r}

# Boston$CMEDV <- log(Boston$medv+1)

#str(Boston)
# categorical hot encoding for rad
Boston$RAD <- as.factor(as.character(Boston$RAD))
dummies<-dummyVars(~RAD,data = Boston)


BostonSc <- Boston %>% select_if(is.numeric) %>% mutate_all(funs(scale)) %>% as.data.frame()

# add binary variable
BostonSc$CHAS <- as.numeric(Boston$CHAS)

##number of NA's
sapply(Boston,function(x){sum(which(is.na(x)))})
# no NA's in data

categorical_1_hot <- predict(dummies,Boston)

# merge data with dummy on RAD
BostonSc <- cbind(BostonSc,categorical_1_hot)

```

We fit a Lasso regression model fixing $\alpha=1$ in glmnet function.

```{r}

sn<-sample.split(BostonSc,0.75)
BostonSc <- BostonSc %>% select(-c(CMEDV))
train <- BostonSc[sn,]
test <- BostonSc[!sn,]
y <- log(Boston$CMEDV)[sn]
testy <- Boston$CMEDV[!sn]

X_train <- train 
X_test <- test 

set.seed(123)  # for reproduce output
model_lasso <- train(x=X_train,y=y,
                  method="glmnet",
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=CARET.TRAIN.CTRL,
                  tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                       lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                            0.00075,0.0005,0.0001)))

```

\newpage

```{r}
# grid search and metrics
model_lasso

```

#### cv.glmnet function with 10 fold CV and default lambdas (100 values)

```{r}
X_train <- data.matrix(X_train)
X_test <- data.matrix(X_test)
set.seed(123)
# by default uses 10 fold cross validation. Uses 100 lambdas
modelglmnet <- cv.glmnet(X_train,y,family = "gaussian",
                         alpha = 1,standardize = FALSE,
                         intercept = TRUE,type.measure = "mse")

# plot(modelglmnet,main = "Lasso")
# coef(modelglmnet, s = "lambda.1se")

cat("The best lambda is",modelglmnet$lambda.min,"\n")
cat("Metric MSE value is:",min(modelglmnet$cvm))

```


#### Glmnet with 10 fold cross-validation and same lambdas as caret 

```{r}
modelglmnet2 <- cv.glmnet(X_train,y,family = "gaussian",
                          alpha = 1,standardize = FALSE,intercept = TRUE,lambda = c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),0.00075,0.0005,0.0001),
                          type.measure = "mse")
```


Now we make predictions with glmnet. 10 fold CV is performed with $\alpha=1$ for Lasso regression. Intercept is set to True because __response__ is not centered (we apply log transformation).

```{r}
pred_glmnet = exp(predict(modelglmnet2,newx=X_test))
pred_caret = exp(predict(model_lasso,newdata = X_test))
# cbind(pred,exp(testy)-1) %>% View()

data_pred <- data.frame(pred_glmnet=pred_glmnet,pred_caret=pred_caret)
data_pred %>%
ggplot(.,aes(pred_caret,pred_glmnet)) + geom_point() +
ggtitle("Prediction using caret vs. glmnet")


```

We observe that our predictions are very correlated. Now we examine prediction of glmnet model (using default lambdas) vs. real values. We have problems with high values, we predict 30 to 40 and real value are 40 to 50, but in interval 10-30 we have good performance in general terms.

```{r}
# prepare predict object 
pred_glmnet0 = exp(predict(modelglmnet,newx=X_test))
data_pred0 <- data.frame(pred_glmnet0=pred_glmnet0,response=(testy))

data_pred0 %>%   ggplot(.,aes(pred_glmnet0,response)) + geom_point() + 
ggtitle("Predict with default 100 lambdas vs. CMEDV variable")
```

\newpage

#### MSE metric obtained in 3 methods

```{r}
# nice package for calculate a lot of metrics:
require(Metrics)
cat("With tune lambdas")
mse(pred_glmnet,testy)
cat("With default lambdas")
mse(pred_glmnet0,testy)
cat("Using caret package and tuning lambdas:")
mse(pred_caret,testy)

cat("The best metric is obtained with caret and in second 
    place with default lambdas using glmnet package. \n")
```

#### Interpretation of estimated best model ( object __modelglmnet__ )

```{r}

cat("Plot lambdas vs. metric")
plot(modelglmnet,main = "Lasso")

cat("Coef. with best lambda")
coef(modelglmnet, s = "lambda.min")

imp_coef <- as.data.frame(as.matrix(coef(modelglmnet, s = "lambda.min")))
imp_coef$coef.name <- rownames(imp_coef)
imp_coef$coef.value <- imp_coef$`1`

ggplot(imp_coef) +
    geom_bar(aes(x=reorder(coef.name,coef.value),y=coef.value),
             stat="identity") +
    ylim(min(imp_coef$coef.value)-.5,max(imp_coef$coef.value)+.5) +
    coord_flip() +
    ggtitle("Coefficents in the Lasso Model with lambda.min argument") +
    theme(axis.title=element_blank())

imp_coef <- as.data.frame(as.matrix(coef(modelglmnet, s = "lambda.1se")))
imp_coef$coef.name <- rownames(imp_coef)
imp_coef$coef.value <- imp_coef$`1`

ggplot(imp_coef) +
    geom_bar(aes(x=reorder(coef.name,coef.value),y=coef.value),
             stat="identity") +
    ylim(min(imp_coef$coef.value)-.5,max(imp_coef$coef.value)+.5) +
    coord_flip() +
    ggtitle("Coefficents in the Lasso Model with lambda.1se argument") +
    theme(axis.title=element_blank())



cat("Coef. with lambda in 1 square error")
coef(modelglmnet, s = "lambda.1se")
cat("Notice that loosing few value of metric we obtain a model 
with less parameters, \n indicating that lasso obtain sparsity solutions.")

```

### Ap 2.

Perform ridge regression and compare with the ridge realised in previous practice.
To perform ridge regression we fix the value of $\alpha=0$.

```{r}
# in this exercise we go to use log response.
y <- log(Boston$CMEDV)
X_features <- Boston[,-14]
X_features <- sapply(X_features,as.numeric)
X_features <- as.matrix(X_features)

set.seed(123)
modelglmnet_ridge <- cv.glmnet(x=X_features,y = y ,
family = "gaussian",alpha = 0,
standardize = TRUE,intercept = TRUE,type.measure = "mse",nfolds = 10)

```

```{r,echo=TRUE}

CV_kfolds_ridge <- function(X,y,folds,lambda.v,seed_value=NULL) {
  
  # as.numeric all covariables X
  X <- sapply(X,as.numeric)
  
  # scale data & input data to -1
  X[is.na(X)] <- -1
  mean_X <- apply(X,2,mean)
  sd_X <- apply(X,2,sd)
  y_mean <- mean(y)
  
  X <- as.matrix(scale(X, center=mean_X, scale=sd_X))
  y <- as.matrix(scale(y, center=y_mean, scale=FALSE))
  
  data <- cbind(y,X)
  
  if(is.null(seed_value)){1234}
  set.seed(seed_value)
  # permut data and make k folds
  data<-data[sample(nrow(data)),]
  
  
  # Create k size folds
  kfolds <- cut(seq(1,nrow(data)),breaks=folds,labels=FALSE)
  
  
  # result for lambda l
  cv_kfolds_mpse_lambda <- numeric(length(lambda.v))
  
  
  for (l in 1:length(lambda.v)){ 
    
    lambda <- lambda.v[l]
    cv_kfolds_mpse <- numeric(folds)
    
    for (i in 1:folds){
    
    idx <- which(kfolds==i)
    train <- data[-idx, ]
    test <- data[idx, ]
    n_train <- dim(train)[1]
    p_train <- dim(train)[2]-1
    n_test <- dim(test)[1]
    p_test <- dim(test)[2]-1
    
    
    beta.path <- matrix(0,nrow=1, ncol=p_train)
    
    
    XtX <- t(train[,-1]) %*% train[,-1]
    H.lambda.aux <- t(solve(XtX + lambda*diag(1,p_train))) %*% t(train[,-1])
    beta.path <-  H.lambda.aux %*% as.matrix(train[,1])
    hat.Y_val <- test[,-1] %*% beta.path
    cv_kfolds_mpse[i] <- sum((test[,1]-hat.Y_val)^2)/n_test
    }
    
    cv_kfolds_mpse_lambda[l] <- mean(cv_kfolds_mpse)
    cv_kfolds_mpse <- NULL
  }
  
  
  min_lambda <- lambda.v[which.min(cv_kfolds_mpse_lambda)]
  cat("Resultados considerando k=",folds,"folds: \n")
  cat("\n")
  cat("El lambda que hace mínimo el PMSE es: ",min_lambda,"\n")
  cat("\n")
  cat("El PMSE es: ",min(cv_kfolds_mpse_lambda) )
  
  plot(lambda.v,cv_kfolds_mpse_lambda,main = 'Valores lambda vs. k-fold MSPE',
       ylab='MPSE error',xlab="values of lambda")
  
}

lambda.search = modelglmnet_ridge$lambda

 
# CV_kfolds_ridge(X=Boston[,-14],y=log(Boston$CMEDV),
#                 folds=10,lambda.v=lambda.search,seed_value = 123)

```


\newpage

#### Compare methods

Using 10 fold cv (with seed 123), with our function we obtain:

```{r}

lambda.search = modelglmnet_ridge$lambda

CV_kfolds_ridge(X=Boston[,-14],y=log(Boston$CMEDV),folds=10,lambda.v=lambda.search,seed_value = 123)


```

With glmnet function we obtain:

```{r}

cat("PMSE with glmnet is:",min(modelglmnet_ridge$cvm),"\n")
plot(modelglmnet_ridge)

```

With two methods, obtain closest PMSE.

